{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96d8805",
   "metadata": {},
   "source": [
    "# Introduction to Gradient Boosting\n",
    "\n",
    "Gradient Boosting is one of the most powerful techniques for building predictive models.\n",
    "\n",
    "The idea of boosting came out of the idea of whether a weak learner can eb modified to become better. The first realization of bosoting saw great success in application of Adaptive Boosting or AdaBoost for short. The weak learners in AdaBoost are decision trees with a single split, called decsion stumpness (for their shortness).\n",
    "\n",
    "Adaptive Boosting and related algorithms were recast i statistical framework and became known as Gradient Boosting Machines. The statistical framework cast boosting asa  numerical optimization problem where the objective is to minimize the loss of he model by ading weak learners using a gradient like procedure and hence the name,\n",
    "\n",
    "The Gradient Bosoting algorithm involves 3 elements:\n",
    "\n",
    "1) A loss function to be optimized - such as cross entropy for classification or mean squared error for regression problems\n",
    "2) A weak learner to make prdictions such a single split decsion tree / decision stump\n",
    "3) An additive model used to add weak learners to minimize the loss function\n",
    "\n",
    "New weaka learners are added to the model in an effort to correct the residual errors of the previous trees. The result is a powerful predictive modelling algorithm perhaps more powerful than Random Forests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
